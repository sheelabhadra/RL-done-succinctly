{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to MDPs\n",
    "\n",
    "- MDPs formalyy describe the environment for RL\n",
    "- The environment is fully observable\n",
    "- i.e. The current state completely characterizes the process (the way the environment unfolds depends on a state and we know that state)\n",
    "- Almost all RL problems can be formalized as MDPs e.g.\n",
    "    - Optimal control primarily deals with continuous MDPs (octopus swimming in through fluid)\n",
    "    - Partially observable problems can be converted into MDPs\n",
    "    - Bandits are MDPs with one state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Transition Matrix\n",
    "\n",
    "For a Markov state $s$ and successor state $s'$, the state transition probability is given by\n",
    "$$P_{ss'} = \\mathbb P[S_{t+1} = s' | S_{t} = s]$$  \n",
    "\n",
    "Since we can observe all the states, we can build the state transition matrix containing the transition probability from all states $s$ to all successor states $s'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Process\n",
    "\n",
    "A Markov process (or Markov Chain) is a memoryless random process, i.e. a sequence of random states $S_{1}, S_{2}, ...$ with the Markov property. It is denoted by a tuple $(S, T)$.\n",
    "* $S$ is a finite set of states \n",
    "* $P$ is the transition probability matrix, $P_{ss'} = \\mathbb P[S_{t+1} = s' | S_{t} = s]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Reward Process\n",
    "\n",
    "A Markov reward process is a Markov Chain with values. It is denoted by a tuple $(S, T, R, \\gamma)$.\n",
    "* $S$ is a finite set of states \n",
    "* $P$ is the transition probability matrix, $P_{ss'} = \\mathbb P[S_{t+1} = s' | S_{t} = s]$\n",
    "* $R$ is a immediate reward function, $R_{s} = \\mathbb E[R_{t+1}|S_{t} = s]$\n",
    "* $\\gamma$ is a discount factor, $\\gamma \\in [0,1]$\n",
    "\n",
    "#### Return\n",
    "\n",
    "The return $G_{t}$ (a random variable - one sample) is the total discounted reward from time step $t$.\n",
    "$$G_{t} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$$\n",
    "* $\\gamma$ close to 0 leads to myopic evaluation\n",
    "* $\\gamma$ close to 1 leads to far-sighted evaluation\n",
    "* Discount is important because our model might not be perfect\n",
    "* Discout also avoids infinite retrurns in cyclic Markov processes\n",
    "\n",
    "#### Value Function\n",
    "\n",
    "The value function $v(s)$ gives the long term value of state $s$. The state value function of an MRP is the expected return starting from state $s$\n",
    "$$v(s) = \\mathbb E[G_{t} | S_{t} = s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation for MRPs\n",
    "\n",
    "The value function can be decomposed into 2 parts:\n",
    "* immediate reward $R_{t+1}$\n",
    "* discounted value of successor state $\\gamma v(S_{t+1})$\n",
    "\n",
    "This is helpful in formulating RL problems as dynamic programming problems.\n",
    "\n",
    "<img src=\"Figures/02-bellman-equation.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/02-value-function-computation.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/02-bellman-matrix.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/02-solving-bellman-matrix.png\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process\n",
    "\n",
    "A Markov decision process is a Markov reward process with decisions. It is an environment in which all states are Markov. It is denoted by a tuple $(S, A, T, R, \\gamma)$.\n",
    "* $S$ is a finite set of states \n",
    "* $A$ is a finite set of actions\n",
    "* $P$ is the transition probability matrix, $P_{ss'}^{a} = \\mathbb P[S_{t+1} = s' | S_{t} = s, A_{t} = a]$\n",
    "* $R$ is a immediate reward function, $R_{s}^{a} = \\mathbb E[R_{t+1}|S_{t} = s, A_{t} = a]$\n",
    "* $\\gamma$ is a discount factor, $\\gamma \\in [0,1]$\n",
    "\n",
    "#### Policies\n",
    "\n",
    "A policy $\\pi$ is a distribution over actions given states,\n",
    "$$\\pi(a/s) = \\mathbb P[A_{t} = a | S_{t} = s]$$\n",
    "e.g. if the agent is in a given state, map from that state its probability of going left or going right.\n",
    "* A policy fully defines the behavior of an agent\n",
    "* MDP policies depend on the current state (not the history)\n",
    "* i.e. Policies are stationary (time-independent)\n",
    "* There are no rewards in the equation because the state fully characterizes the future rewards. We only need to look at what state we are in and what actions should we pick in a way that will get us the most future reward.\n",
    "\n",
    "#### Value Function\n",
    "\n",
    "The *state-value function* $v_{\\pi}(s)$ of an MDP is the expected return starting from state $s$, and then following policy $\\pi$.\n",
    "$$v_{\\pi}(s) = \\mathbb E[G_{t} | S_{t} = s]$$\n",
    "\n",
    "The *action-value function* $q_{\\pi}(s,a)$ is the expected return starting from state $s$, taking action $a$, and then following policy $\\pi$. It tells us how good a action is given that we are in a particular state.\n",
    "$$q_{\\pi}(s,a) = \\mathbb E[G_{t}|S_{t} = s, A_{t} = a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bellman Expectation Equation\n",
    "\n",
    "$V_{\\pi}(S_{t+1})$: The value of the successive step if we know that we will be following the policy $\\pi$ from that state onwards. So, we look up the value of one step of policy and then we ask how much more value we will get following the same policy.  \n",
    "\n",
    "$V_{\\pi}$ tells us how good it is to be in a particular state.  \n",
    "\n",
    "$Q_{\\pi}$ tells us how good it is to take a particular action from a given state.\n",
    "\n",
    "\n",
    "<img src=\"Figures/02-bellman-expectation-equation.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/02-bellman-state-value.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/02-bellman-state-value-2.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/02-bellman-action-value.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/02-bellman-action-value-2.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/02-student-MDP.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "For the student MDP example, the policy is that the probability of choosing an action from a particular state is 50-50. It shows the calculation of the *state-value function*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"Figures/02-bellman-expectation-equation-matrix.png\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Value Function\n",
    "\n",
    "The *optimal state-value fuction* $v_{*}(s)$ is the maximum state-value function over all policies \n",
    "$$ v_{*}(s) = \\underset{\\pi}{\\max} v_{\\pi}(s)$$\n",
    "\n",
    "The *optimal action-value fuction* $q_{*}(s,a)$ is the maximum action-value function over all policies \n",
    "$$ q_{*}(s,a) = \\underset{\\pi}{\\max} q_{\\pi}(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Policy\n",
    "\n",
    "Define a partial ordering over policies  \n",
    "if $v_{\\pi}(s) \\geq v_{\\pi '}(s), \\forall s$  \n",
    "then $\\pi \\geq \\pi'$\n",
    "\n",
    "#### Theorem\n",
    "For any MDP\n",
    "* There exists an optimal policy $\\pi_{*}$ that is better than or equal to all other policies, $\\pi_{*} \\geq \\pi, \\forall \\pi$\n",
    "* All optimal policies achieve the optimal value function, $v_{\\pi *}(s) = v_{*}(s)$\n",
    "* All optimal policies achieve the optimal action-value function, $q_{\\pi *}(s,a) = q_{*}(s,a)$\n",
    "\n",
    "<img src=\"Figures/02-optimal-policy.png\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/02-bellman-optimality-equation.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "Instead of taking the average over the action-values, we take the max of the action-values. We look at the value of all the actions we can take and pick the max out of them. This tells us how good it is to be in the state.\n",
    "\n",
    "<img src=\"Figures/02-bellman-optimality-equation.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/02-bellman-optimality-equation-Q.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/02-bellman-optimality-equation-v-2.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/02-bellman-optimality-equation-Q-2.png\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving the Bellman Optimality Equation\n",
    "\n",
    "* Bellman Optimality Equation is non-linear\n",
    "* No closed form solution (in general)\n",
    "* Many iterative solution methods:\n",
    "    * Value iteration\n",
    "    * Policy iteration\n",
    "    * Q-learning\n",
    "    * Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tamids]",
   "language": "python",
   "name": "conda-env-tamids-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
