{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to MDPs\n",
    "\n",
    "- MDPs formalyy describe the environment for RL\n",
    "- The environment is fully observable\n",
    "- i.e. The current state completely characterizes the process (the way the environment unfolds depends on a state and we know that state)\n",
    "- Almost all RL problems can be formalized as MDPs e.g.\n",
    "    - Optimal control primarily deals with continuous MDPs (octopus swimming in through fluid)\n",
    "    - Partially observable problems can be converted into MDPs\n",
    "    - Bandits are MDPs with one state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Transition Matrix\n",
    "\n",
    "For a Markov state $s$ and successor state $s'$, the state transition probability is given by\n",
    "$$P_{ss'} = \\mathbb P[S_{t+1} = s' | S_{t} = s]$$  \n",
    "\n",
    "Since we can observe all the states, we can build the state transition matrix containing the transition probability from all states $s$ to all successor states $s'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Process\n",
    "\n",
    "A Markov process (or Markov Chain) is a memoryless random process, i.e. a sequence of random states $S_{1}, S_{2}, ...$ with the Markov property. It is denoted by a tuple $(S, T)$.\n",
    "* $S$ is a finite set of states \n",
    "* $P$ is the transition probability matrix, $P_{ss'} = \\mathbb P[S_{t+1} = s' | S_{t} = s]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Reward Process\n",
    "\n",
    "A Markov reward process is a Markov Chain with values. It is denoted by a tuple $(S, T, R, \\gamma)$.\n",
    "* $S$ is a finite set of states \n",
    "* $P$ is the transition probability matrix, $P_{ss'} = \\mathbb P[S_{t+1} = s' | S_{t} = s]$\n",
    "* $R$ is a immediate reward function, $R_{s} = \\mathbb E[R_{t+1}|S_{t} = s]$\n",
    "* $\\gamma$ is a discount factor, $\\gamma \\in [0,1]$\n",
    "\n",
    "#### Return\n",
    "\n",
    "The return $G_{t}$ (a random variable - one sample) is the total discounted reward from time step $t$.\n",
    "$$G_{t} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$$\n",
    "* $\\gamma$ close to 0 leads to myopic evaluation\n",
    "* $\\gamma$ close to 1 leads to far-sighted evaluation\n",
    "* Discount is important because our model might not be perfect\n",
    "* Discout also avoids infinite retrurns in cyclic Markov processes\n",
    "\n",
    "#### Value Function\n",
    "\n",
    "The value function $v(s)$ gives the long term value of state $s$. The state value function of an MRP is the expected return starting from state $s$\n",
    "$$v(s) = \\mathbb E[G_{t} | S_{t} = s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation for MRPs\n",
    "\n",
    "The value function can be decomposed into 2 parts:\n",
    "* immediate reward $R_{t+1}$\n",
    "* discounted value of successor state $\\gamma v(S_{t+1})$\n",
    "\n",
    "This is helpful in formulating RL problems as dynamic programming problems.\n",
    "\n",
    "<img src=\"Figures/02-bellman-equation.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/02-value-function-computation.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/02-bellman-matrix.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/02-solving-bellman-matrix.png\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tamids]",
   "language": "python",
   "name": "conda-env-tamids-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
