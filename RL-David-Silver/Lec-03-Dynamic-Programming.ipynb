{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Planning by Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dynamic programming assumes full knowledge of the MDP\n",
    "* It is used for *planning* in an MDP\n",
    "* For prediction:\n",
    "    * Input: MDP $<S, A, P, R, \\gamma>$\n",
    "    * or: MRP $<S, P^{\\pi}, R^{\\pi}, \\gamma>$\n",
    "    * Output: value function $v_{\\pi}$\n",
    "* Or for control:\n",
    "    * Input: MDP $<S, A, P, R, \\gamma>$\n",
    "    * Output: optimal value function $v_{*}$\n",
    "    * and: optimal policy $\\pi_{*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Policy Evaluation\n",
    "\n",
    "* Problem: evaluate a given policy $\\pi$\n",
    "* Solution: iterative application of Bellman expectation backup\n",
    "* $v_{1} \\rightarrow v_{2} \\rightarrow ... \\rightarrow v_{\\pi}$\n",
    "* Using synchronous backups (we look at all the states for a value function and apply iterative update to obtain completely new value function for all the states i.e. we consider all states in each step)\n",
    "    * At each iteration $k+1$\n",
    "    * For all states $s \\in S$\n",
    "    * Update $v_{k+1}(s)$ from $v_{k}(s')$\n",
    "    * where $s'$ is a successor state of $s$\n",
    "\n",
    "<img src=\"Figures/03-iterative-policy-evaluation.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/03-gridworld.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/03-policy-evaluation-1.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/03-policy-evaluation-2.png\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Policy Iteration\n",
    "\n",
    "* Given a policy $\\pi$\n",
    "    * *Evaluate* the policy $\\pi$\n",
    "    $$ v_{\\pi}(s) = \\mathbb E[R_{t+1} + \\gamma R_{t+2} + ... | S_{t} = s]$$\n",
    "    * *Improve* the policy by acting greedily with respect to $v_{\\pi}$\n",
    "    $$ \\pi' = greedy(v_{\\pi})$$\n",
    "    \n",
    "<img src=\"Figures/03-policy-iteration.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/03-policy-improvement.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/03-policy-improvement-2.png\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Policy Iteration\n",
    "\n",
    "* We can use a stopping condition or simply stop after $k$ iterations of iterative policy evaluation\n",
    "* $\\epsilon$-convergence of value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "Any optimal policy can be subdivided into two components:\n",
    "* An optimal first action $A_{*}$\n",
    "* Followed by an optimal policy from successor state $S'$\n",
    "\n",
    "#### Principle of Optimality \n",
    "A policy $\\pi(a|s)$ achieves the optimal value from state $s$, $v_{\\pi}(s) = v_{*}(s)$, iff\n",
    "* For any state $s'$ reachable from $s$\n",
    "* $\\pi$ achieves the optimal value from state $s'$, $v_{\\pi}(s') = v_{*}(s')$\n",
    "\n",
    "e.g. for each state the wind might blow us to ($s'$) that policy would behave optimally from that state onwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value iteration in MDPs\n",
    "\n",
    "* Problem: find optimal policy $\\pi$\n",
    "* Solution: iterative application of Bellman optimality backup\n",
    "* $v_{1} \\rightarrow v_{2} \\rightarrow ... \\rightarrow v_{*}$\n",
    "* Using synchronous backups\n",
    "    * At each iteration $k+1$\n",
    "    * For all states $s \\in S$\n",
    "    * Update $v_{k+1}(s)$ from $v_{k}(s')$\n",
    "* Unlike policy iteration, there is no explicit policy\n",
    "* Intermediate value functions may not correspond to any policy\n",
    "\n",
    "<img src=\"Figures/03-value-iteration.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/03-async-dp.png\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tamids]",
   "language": "python",
   "name": "conda-env-tamids-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
