{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Model-Free Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Reinforcement Learning\n",
    "\n",
    "* MC methods learn directly from episodes of experience\n",
    "* MC is *model-free*: no knowledge of MDP transitions/rewards\n",
    "* MC learns from complete episodes\n",
    "* Value function = mean return\n",
    "* Caveat: can only apply MC to *episodic* MDPs\n",
    "    * All episodes must terminate\n",
    "   \n",
    "#### Monte-Carlo Policy Evaluation\n",
    "\n",
    "* Goal: learn $v_{\\pi}$ from epidodes of experience under policy $\\pi$\n",
    "$$S_{1}, A_{1}, R_{2}, ..., S_{k} \\sim \\pi$$\n",
    "* Monte-Carlo policy evaluation uses *empirical mean* return instead of *expected* return\n",
    "\n",
    "#### First-Visit Monte-Carlo Policy Evaluation\n",
    "\n",
    "* To evaluate state $s$\n",
    "* The first time-step $t$ that state $s$ is visited in an episode,\n",
    "* Increment counter $N(s) \\leftarrow N(s) + 1$\n",
    "* Increment total return $S(s) \\leftarrow S(s) + G_{t}$\n",
    "* Value is estimated by mean return $V(s) = S(s)/N(s)$\n",
    "* By law of large numbers, $V(s) \\rightarrow v_{\\pi}(s)$ as $N(s) \\rightarrow \\infty$\n",
    "> In this case we don't explore the whole state space and only traverse the states that can be covered under a given policy. This leads to sampling, thus reducing the search space.\n",
    "\n",
    "#### Every-Visit Monte-Carlo Policy Evaluation\n",
    "\n",
    "* To evaluate state $s$\n",
    "* Every time-step $t$ that state $s$ is visited in an episode,\n",
    "* Increment counter $N(s) \\leftarrow N(s) + 1$\n",
    "* Increment total return $S(s) \\leftarrow S(s) + G_{t}$\n",
    "* Value is estimated by mean return $V(s) = S(s)/N(s)$\n",
    "* By law of large numbers, $V(s) \\rightarrow v_{\\pi}(s)$ as $N(s) \\rightarrow \\infty$\n",
    "\n",
    "<img src=\"Figures/04-incremental-mean.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/04-incremental-mc.png\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Temporal-Difference Learning\n",
    "\n",
    "* TD methods learn directly from episodes of experience\n",
    "* TD is *model-free:* no knowledge of the transitions/rewards\n",
    "* TD learns from *incomplete* episodes, by *bootstrapping*: i.e. we don't need to go all the way until hit the wall and compute the reward obtained from the complete trajectory. We could take a partial trajectory and make an estimate of the reward from the current state up until the wall in place of the return. This idea is called bootstrapping in which we make an estimate of what will happen in the remainder of the trajectory from that point onwards. We update our guess of our reward. (Fundamental concept behind TD learning)\n",
    "\n",
    "#### n-Step Prediction\n",
    "* Let TD target look *n* steps into the future\n",
    "* *n* step return\n",
    "$$ G_{t}^{(n)} = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma ^{n-1}R_{t+n} + \\gamma ^{n}V(S_{t+n})$$\n",
    "\n",
    "#### Averaging n-Step Returns\n",
    "* We can average n-step returns over different *n*\n",
    "* We can efficiently combine information from all time-steps\n",
    "\n",
    "#### $\\lambda$-return\n",
    "* Using weight $(1-\\lambda)\\lambda ^{n-1}$\n",
    "$$G_{t}^{\\lambda} = (1-\\lambda)\\sum_{n=1}^{\\infty}\\lambda^{n-1}G_{t}^{(n)} $$\n",
    "* Forward-view TD($\\lambda$)\n",
    "$$V(S_{t}) \\leftarrow V(S_{t}) + \\alpha(G_{t}^{\\lambda} - V(S_{t})) $$\n",
    "\n",
    "<img src=\"Figures/04-eligibility-trace.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "<img src=\"Figures/04-backward-view.png\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
