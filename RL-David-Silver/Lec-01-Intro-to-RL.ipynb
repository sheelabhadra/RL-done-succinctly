{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is reinforcement learning different?\n",
    "\n",
    "- There's no supervisor, only a *reward* signal\n",
    "    - There's no immediate feedback on whether the decision was good or bad\n",
    "- Feedback is delayed, not instantaneous\n",
    "    - The feedback is obtained many steps later to know whether the set of decisions were good or bad\n",
    "- Time really matters (sequential, non i.i.d data)\n",
    "    - An agent moves through a dynamic system where things observed at a time instant are correlated with the things observed in the immediate next time instant\n",
    "- Agent's actions affect the subsequent data it receives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Areas of application\n",
    "\n",
    "- Robotics: fly stunt manuevers in a helicopter (reward- good/bad)\n",
    "- Video Games: Backgammon (reward- score)\n",
    "- Investment and trading: manage an investment portfolio (reward- money)\n",
    "- Machine control: control a power station, wind-mill (reward- efficient energy generation)\n",
    "- Humanoid robots: make a humanoid robot walk (reward- distance walked/falling over)\n",
    "- Artificial General Intelligence (AGI) or general AI: play many Atari games better than humans (reward- score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "- A reward $R_{t}$ is a scalar feedback signal\n",
    "- Indicates how well agent is doing at step $t$\n",
    "- The agent's job is to maximize the cumulative reward\n",
    "\n",
    "> **Reward Hypothesis:** All goals can be described by the maximization of expected cuulative reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Decision Making\n",
    "\n",
    "- Goal: select actions to maximize total future reward\n",
    "- Actions may have long term consequences\n",
    "- Reward may be delayed\n",
    "- It may be better to sacrifice immediate reward to gain more long-term reward\n",
    "- Examples:\n",
    "    - A financial investment (may take months to mature)\n",
    "    - Refuelling a helicopter (might prevent a crash in several hours)\n",
    "    - Blocking opponent moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent and Environment\n",
    "\n",
    "<img src=\"Figures/01-agent-and-environment.png\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History and State\n",
    "\n",
    "- *History* ($H_{t}$) is the sequence of observations, actions, rewards.\n",
    "- *State* ($S_{t}$) is the information used to determine what happens next.\n",
    "$$S_{t} = f(H_{t})$$  \n",
    "\n",
    "**Environment state $S_{t}^{e}$:** what happens next from the environment's POV based on the history. It is not usually visible to the agent as it doesn't know everything that is present in the environment. Even if $S_{t}^{e}$ is visible, it may contain irrelevant information.  \n",
    "\n",
    "**Agent state $S_{t}^{a}$:** agent's internal representation. These are the set of numbers are used by the reinforcement learning algorithm. These numbers capture what has happened with the agent so far and summarize everything the agent has seen so far. This information is used by the agent to pick the next action.  \n",
    "\n",
    "**Information state (Markov state):** A state $S_{t}$ is Markov iff\n",
    "$$\\mathbb{P}[S_{t+1} | S_{t}] = \\mathbb{P}[S_{t+1} | S_{1},...,S_{t}]$$\n",
    "The future is independent of the past given the present. Once the state is known, the history may be thrown away.  \n",
    "- The environment state $S_{t}^{e}$ is Markov.\n",
    "- The history $H_{t}$ is Markov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Fully Observable Environments\n",
    "\n",
    "Agent directly observes environment state\n",
    "$$O_{t} = S_{t}^{a} = S_{t}^{e}$$\n",
    "- Agent state = environment state = information state\n",
    "- This is a Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partially Observable Environments\n",
    "\n",
    "- Agent indirectly observes environment\n",
    "    - A robot with camera vision isn't told its absolute location\n",
    "    - A trading agent only observes current prices\n",
    "    - A poker playing agent only observes public cards\n",
    "- Agent state $\\neq$ environment state\n",
    "- POMDP\n",
    "- Agent must construct its own state representation $S_{t}^{a}$, e.g.\n",
    "    - Complete history: $S_{t} = H_{t}$\n",
    "    - Beliefs of $S_{t}^{e}$: $S_{t}^{a} = (\\mathbb{P}[S_{t}^{e} = s^{1}],...,\\mathbb{P}[S_{t}^{e} = s^{n}])$\n",
    "    - RNN: $S_{t}^{a} = \\sigma(S_{t-1}^{a}W_{s} + O_{t}W_{o})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Components of an RL Agent\n",
    "\n",
    "- An RL agent may include one or more of these components:\n",
    "    - Policy: agent's behaviour function. It tells us about how the agent picks its actions.\n",
    "    - Value function: how good is each state and/or action. How much reward can we expect to get if we take an action in that particular state.\n",
    "    - Model: agent's representation of the environment. It is about how the agent thinks that the environment works.  \n",
    "\n",
    "<img src=\"Figures/01-maze-example.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "#### Policy\n",
    "- Map from state to action, e.g.\n",
    "- Deterministic policy: $a = \\pi(s)$\n",
    "- Stochastic policy: $\\pi(a | s) = \\mathbb{P}[A = a | S = s]$  \n",
    "\n",
    "<img src=\"Figures/01-maze-policy.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "#### Value function\n",
    "- Prediction of future reward (i.e. expected future total reward)\n",
    "- Used to evaluate the goodness/badness of states\n",
    "$$v_{\\pi}(s) = \\mathbb{E_{\\pi}}[R_{t} + \\gamma R_{t+1} + \\gamma ^{2}R_{t+2} + ... | S_{t} = s ]$$\n",
    "$\\gamma$: discounting factor (indicates that we care more about immediate rewards)  \n",
    "\n",
    "<img src=\"Figures/01-maze-value-function.png\" style=\"width: 550px;\"/>\n",
    "\n",
    "#### Model\n",
    "- A model predicts what the environment will do next\n",
    "- Transitions: $P$ predicts the next state (i.e dynamics)\n",
    "- Rewards: $R$ predicts the next (immediate) reward, e.g.\n",
    "$$P_{ss'}^{a} = \\mathbb{P}[S' = s' | S = s, A = a] $$\n",
    "$$R_{s}^{a}  = \\mathbb{E}[R | S = s, A = a]$$\n",
    "- It is not mandatory to build a model for the environment. But it is something that can be used.  \n",
    "\n",
    "<img src=\"Figures/01-maze-model.png\" style=\"width: 550px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of RL agents\n",
    "\n",
    "- Value based\n",
    "    - No policy (implicit)\n",
    "    - Value function\n",
    "    - The policy can be obtained from the value function\n",
    "- Policy based\n",
    "    - Policy\n",
    "    - No value function\n",
    "    - Maintains a data structure of the arrows\n",
    "- Actor Critic\n",
    "    - Policy\n",
    "    - Value Function\n",
    "- Model Free\n",
    "    - Policy and/or Value Function\n",
    "    - No Model\n",
    "- Model Based\n",
    "    - Policy and/or Value Fucntion\n",
    "    - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning and Planning\n",
    "\n",
    "<img src=\"Figures/01-learning-planning.png\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration vs Exploitation tradeoff\n",
    "\n",
    "- *Exploration* finds more information about the environment\n",
    "- *Exploitation* exploits known information to maximize reward\n",
    "- e.g.: Restaurant Selection\n",
    "    - Exploitation: Go to your favorite restaurant\n",
    "    - Exploration: Try a new restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Control\n",
    "\n",
    "- Predicition: evaluate the future\n",
    "    - Given a policy\n",
    "- Control: optimize the future\n",
    "    - Find the best policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tamids]",
   "language": "python",
   "name": "conda-env-tamids-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
